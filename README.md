# openGPTo1

[![openGPTo1](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/aJupyter/Awesome-LLM-paper)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/aJupyter/Awesome-LLM-paper)

This repository contains papers related to the GPTo1.

We strongly encourage researchers in the hope of advancing their excellent work.

---

## Contents [Papers](#papers)

- 
- 
- Anthony, Thomas, Zheng Tian, and David Barber. 2017. “Thinking Fast and
Slow with Deep Learning and Tree Search.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/1705.08439>.

- Brown, Bradley, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le,
Christopher Ré, and Azalia Mirhoseini. 2024. “Large Language Monkeys:
Scaling Inference Compute with Repeated Sampling.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2407.21787>.

- Brown, Noam, and Tuomas Sandholm. 2017. “Libratus: The Superhuman AI for
No-Limit Poker.” In *Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence*. California: International Joint
Conferences on Artificial Intelligence Organization.
<https://www.onlinecasinoground.nl/wp-content/uploads/2018/10/Libratus-super-human-no-limit-poker-Sandholm-Brown.pdf>.

- Chen, Ziru, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan
Sun. 2024. “When Is Tree Search Useful for LLM Planning? It Depends on
the Discriminator.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2402.10890>.

- Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
Lukasz Kaiser, Matthias Plappert, et al. 2021. “Training Verifiers to
Solve Math Word Problems.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2110.14168>.

- Feng, Xidong, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen,
Weinan Zhang, and Jun Wang. 2023. “Alphazero-Like Tree-Search Can Guide
Large Language Model Decoding and Training.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2309.17179>.

Gandhi, Kanishk, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng,
Archit Sharma, and Noah D Goodman. 2024. “Stream of Search (SoS):
Learning to Search in Language.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2404.03683>.

Gulcehre, Caglar, Tom Le Paine, Srivatsan Srinivasan, Ksenia
Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, et al.
2023. “Reinforced Self-Training (ReST) for Language Modeling.” *arXiv
\[Cs.CL\]*.
<https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=7hwJ2ckAAAAJ:evX43VCCuoAC>.

Jones, Andy L. 2021. “Scaling Scaling Laws with Board Games.” *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2104.03113>.

Kazemnejad, Amirhossein, Milad Aghajohari, Eva Portelance, Alessandro
Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024.
“VinePPO: Unlocking RL Potential for LLM Reasoning Through Refined
Credit Assignment.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2410.01679>.

Kirchner, Jan Hendrik, Yining Chen, Harri Edwards, Jan Leike, Nat
McAleese, and Yuri Burda. 2024. “Prover-Verifier Games Improve
Legibility of LLM Outputs.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2407.13692>.

Kumar, Aviral, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes,
Avi Singh, Kate Baumli, et al. 2024. “Training Language Models to
Self-Correct via Reinforcement Learning.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2409.12917>.

Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen
Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl
Cobbe. 2023. “Let’s Verify Step by Step.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2305.20050>.

Nakano, Reiichiro, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,
Christina Kim, Christopher Hesse, et al. 2021. “WebGPT: Browser-Assisted
Question-Answering with Human Feedback.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2112.09332>.

Nye, Maxwell, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,
Jacob Austin, David Bieber, David Dohan, et al. 2021. “Show Your Work:
Scratchpads for Intermediate Computation with Language Models.” *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2112.00114>.

Paul G. Allen School. 2024. “Parables on the Power of Planning in AI:
From Poker to Diplomacy: Noam Brown (OpenAI).” Youtube.
<https://www.youtube.com/watch?v=eaAonE58sLU>.

Putta, Pranav, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn,
Divyansh Garg, and Rafael Rafailov. 2024. “Agent Q: Advanced Reasoning
and Learning for Autonomous AI Agents.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2408.07199>.

Setlur, Amrith, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob
Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral
Kumar. 2024. “Rewarding Progress: Scaling Automated Process Verifiers
for LLM Reasoning.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2410.08146>.

Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, et al. 2016. “Mastering
the Game of Go with Deep Neural Networks and Tree Search.” *Nature* 529
(7587): 484–89. <https://www.nature.com/articles/nature16961>.

Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess
and Shogi by Self-Play with a General Reinforcement Learning Algorithm.”
*arXiv \[Cs.AI\]*. <http://arxiv.org/abs/1712.01815>.

Singh, Avi, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush
Patil, Xavier Garcia, Peter J Liu, et al. 2023. “Beyond Human Data:
Scaling Self-Training for Problem-Solving with Language Models.” *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2312.06585>.

Snell, Charlie, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. “Scaling
LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model
Parameters.” *arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2408.03314>.

Su, Dijia, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and
Qinqing Zheng. 2024. “Dualformer: Controllable Fast and Slow Thinking by
Learning with Randomized Reasoning Traces.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2410.09918>.

Uesato, Jonathan, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel,
Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022.
“Solving Math Word Problems with Process- and Outcome-Based Feedback.”
*arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2211.14275>.

Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024.
“Mixture-of-Agents Enhances Large Language Model Capabilities.” *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2406.04692>.

Wang, Peiyi, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli
Chen, Y Wu, and Zhifang Sui. 2023. “Math-Shepherd: Verify and Reinforce
LLMs Step-by-Step Without Human Annotations.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2312.08935>.

Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. “Self-Consistency
Improves Chain of Thought Reasoning in Language Models.” *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2203.11171>.

Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. “Chain-of-Thought
Prompting Elicits Reasoning in Large Language Models.” Edited by S
Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, and A Oh. *arXiv
\[Cs.CL\]*, 24824–37.
<https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf>.

Welleck, Sean, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf,
Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. “From
Decoding to Meta-Generation: Inference-Time Algorithms for Large
Language Models.” *arXiv \[Cs.CL\]*. <http://arxiv.org/abs/2406.16838>.

Wu, Tianhao, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and
Sainbayar Sukhbaatar. 2024. “Thinking LLMs: General Instruction
Following with Thought Generation.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2410.10630>.

Wu, Yangzhen, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
2024. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal
Inference for Problem-Solving with Language Models.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2408.00724>.

Xie, Yuxi, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P
Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. “Monte Carlo Tree
Search Boosts Reasoning via Iterative Preference Learning.” *arXiv
\[Cs.AI\]*. <http://arxiv.org/abs/2405.00451>.

Xie, Yuxi, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian
He, and Qizhe Xie. 2023. “Self-Evaluation Guided Beam Search for
Reasoning.” *arXiv \[Cs.CL\]*. <http://arxiv.org/abs/2305.00633>.

Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate
Problem Solving with Large Language Models.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2305.10601>.

Yarowsky, David. 1995. “Unsupervised Word Sense Disambiguation Rivaling
Supervised Methods.” In *Proceedings of the 33rd Annual Meeting on
Association for Computational Linguistics -*. Morristown, NJ, USA:
Association for Computational Linguistics.
<https://dl.acm.org/doi/10.3115/981658.981684>.

Yoshida, Davis, Kartik Goyal, and Kevin Gimpel. 2024.
“<span class="nocase">MAP’s</span> Not Dead yet: Uncovering True
Language Model Modes by Conditioning Away Degeneracy.” In *Proceedings
of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)*, 16164–215. Stroudsburg, PA, USA:
Association for Computational Linguistics.
<https://aclanthology.org/2024.acl-long.855.pdf>.

Zelikman, Eric, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber,
and Noah D Goodman. 2024. “Quiet-STaR: Language Models Can Teach
Themselves to Think Before Speaking.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2403.09629>.

Zelikman, Eric, Yuhuai Wu, Jesse Mu, and Noah D Goodman. 2022. “STaR:
Bootstrapping Reasoning with Reasoning.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2203.14465>.

Zhao, Stephen, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. 2024.
“Probabilistic Inference in Language Models via Twisted Sequential Monte
Carlo.” *arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2404.17546>.

- [🌟 Contributors](#-contributors)
- [Star History](#star-history)

---

# Resources

## Workshops and Tutorials

<table>
    <tr>
        <th>Theme</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>……</th>
        <th>……</th>
        <th>……</th>
        <th>……</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">……</td>
    </tr>
</table>

# Papers

## STaR (Zelikman et al. 2022) Formulates LLM improvement as retraining on rationales that lead to correct answers. Justified as approximate policy gradient.
STaR (Zelikman et al. 2022) Formulates LLM improvement as retraining on rationales that lead to correct answers. Justified as approximate policy gradient.
1. Data Benchmark
Dataset: The STaR (Self-Taught Reasoner) paper primarily evaluates its model on the CommonsenseQA dataset. CommonsenseQA is a challenging benchmark that tests models on multiple-choice questions requiring commonsense reasoning. Each question has a correct answer that can only be identified by understanding real-world knowledge or relationships.
Performance Comparison: STaR’s performance is compared against several baselines, including:
A few-shot prompting baseline (+35.9% improvement over zero-shot).
State-of-the-art language models trained with large, annotated datasets for reasoning tasks.
Significance: The paper demonstrates that STaR achieves competitive results on CommonsenseQA, achieving around 72.5% accuracy, which is comparable to models fine-tuned on large-scale annotated datasets. STaR’s method only uses few-shot examples, making it highly efficient.
2. Algorithm
Self-Taught Reasoning (STaR): The core algorithm is based on a bootstrapping loop that leverages a model’s existing reasoning ability to iteratively generate rationales (explanations) for each question. Here’s how it works:

Initial Loop: The model is given a few example rationales as a starting point and is prompted to generate rationales for other questions.
Generation and Fine-Tuning: For each question, the model attempts to generate a rationale and predict the answer. If the generated answer is correct, that rationale is saved; if not, the model is given the correct answer and asked to generate a rationale for it. This process is called rationalization.
Iterative Fine-Tuning: The saved rationales from correct answers are added to the training data. The model is then fine-tuned iteratively on this growing dataset, allowing it to improve over time by learning from its own generated rationales.
Rationale Collection and Repetition: This bootstrapping loop repeats, gradually improving the model’s reasoning accuracy and enabling it to solve increasingly complex reasoning tasks.
Key Advantages: By generating rationales for both correct and corrected answers, STaR sidesteps the need for large annotated datasets. Instead, it leverages its generated rationales as pseudo-labels to refine its reasoning ability.

3. Input and Output
Input: The input to STaR consists of questions requiring commonsense reasoning, each with multiple-choice answers. Additionally, a small set of few-shot examples with rationale-based explanations is used to prime the model initially.
Output:
Generated Rationale: For each question, the model generates a step-by-step rationale (chain of thought) explaining its reasoning process.
Predicted Answer: Based on the rationale, the model selects the most likely answer from the multiple choices.
Output Format Example:
Question: "What can be used to carry a small dog? (Choices: (a) swimming pool, (b) cake box, (c) backpack, (d) toy)"
Generated Rationale: "A cake box or toy may not be practical for carrying a dog. A backpack can carry a small dog securely and is designed to hold things."
Answer: "(c) backpack"



# 🌟 Contributors

<a href="">
  <img src="" />
</a>

# Star History

[![Star History Chart]()
