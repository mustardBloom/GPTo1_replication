# openGPTo1

[![openGPTo1](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/aJupyter/Awesome-LLM-paper)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/aJupyter/Awesome-LLM-paper)

This repository contains papers related to the GPTo1.

We strongly encourage researchers in the hope of advancing their excellent work.

---

## Contents [Papers](#papers)

- 
- 
- Anthony, Thomas, Zheng Tian, and David Barber. 2017. ‚ÄúThinking Fast and
Slow with Deep Learning and Tree Search.‚Äù *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/1705.08439>.

- Brown, Bradley, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le,
Christopher R√©, and Azalia Mirhoseini. 2024. ‚ÄúLarge Language Monkeys:
Scaling Inference Compute with Repeated Sampling.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2407.21787>.

- Brown, Noam, and Tuomas Sandholm. 2017. ‚ÄúLibratus: The Superhuman AI for
No-Limit Poker.‚Äù In *Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence*. California: International Joint
Conferences on Artificial Intelligence Organization.
<https://www.onlinecasinoground.nl/wp-content/uploads/2018/10/Libratus-super-human-no-limit-poker-Sandholm-Brown.pdf>.

- Chen, Ziru, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan
Sun. 2024. ‚ÄúWhen Is Tree Search Useful for LLM Planning? It Depends on
the Discriminator.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2402.10890>.

- Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
Lukasz Kaiser, Matthias Plappert, et al. 2021. ‚ÄúTraining Verifiers to
Solve Math Word Problems.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2110.14168>.

- Feng, Xidong, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen,
Weinan Zhang, and Jun Wang. 2023. ‚ÄúAlphazero-Like Tree-Search Can Guide
Large Language Model Decoding and Training.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2309.17179>.

Gandhi, Kanishk, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng,
Archit Sharma, and Noah D Goodman. 2024. ‚ÄúStream of Search (SoS):
Learning to Search in Language.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2404.03683>.

Gulcehre, Caglar, Tom Le Paine, Srivatsan Srinivasan, Ksenia
Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, et al.
2023. ‚ÄúReinforced Self-Training (ReST) for Language Modeling.‚Äù *arXiv
\[Cs.CL\]*.
<https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=7hwJ2ckAAAAJ:evX43VCCuoAC>.

Jones, Andy L. 2021. ‚ÄúScaling Scaling Laws with Board Games.‚Äù *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2104.03113>.

Kazemnejad, Amirhossein, Milad Aghajohari, Eva Portelance, Alessandro
Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024.
‚ÄúVinePPO: Unlocking RL Potential for LLM Reasoning Through Refined
Credit Assignment.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2410.01679>.

Kirchner, Jan Hendrik, Yining Chen, Harri Edwards, Jan Leike, Nat
McAleese, and Yuri Burda. 2024. ‚ÄúProver-Verifier Games Improve
Legibility of LLM Outputs.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2407.13692>.

Kumar, Aviral, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes,
Avi Singh, Kate Baumli, et al. 2024. ‚ÄúTraining Language Models to
Self-Correct via Reinforcement Learning.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2409.12917>.

Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen
Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl
Cobbe. 2023. ‚ÄúLet‚Äôs Verify Step by Step.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2305.20050>.

Nakano, Reiichiro, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,
Christina Kim, Christopher Hesse, et al. 2021. ‚ÄúWebGPT: Browser-Assisted
Question-Answering with Human Feedback.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2112.09332>.

Nye, Maxwell, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,
Jacob Austin, David Bieber, David Dohan, et al. 2021. ‚ÄúShow Your Work:
Scratchpads for Intermediate Computation with Language Models.‚Äù *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2112.00114>.

Paul G. Allen School. 2024. ‚ÄúParables on the Power of Planning in AI:
From Poker to Diplomacy: Noam Brown (OpenAI).‚Äù Youtube.
<https://www.youtube.com/watch?v=eaAonE58sLU>.

Putta, Pranav, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn,
Divyansh Garg, and Rafael Rafailov. 2024. ‚ÄúAgent Q: Advanced Reasoning
and Learning for Autonomous AI Agents.‚Äù *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2408.07199>.

Setlur, Amrith, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob
Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral
Kumar. 2024. ‚ÄúRewarding Progress: Scaling Automated Process Verifiers
for LLM Reasoning.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2410.08146>.

Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, et al. 2016. ‚ÄúMastering
the Game of Go with Deep Neural Networks and Tree Search.‚Äù *Nature* 529
(7587): 484‚Äì89. <https://www.nature.com/articles/nature16961>.

Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. ‚ÄúMastering Chess
and Shogi by Self-Play with a General Reinforcement Learning Algorithm.‚Äù
*arXiv \[Cs.AI\]*. <http://arxiv.org/abs/1712.01815>.

Singh, Avi, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush
Patil, Xavier Garcia, Peter J Liu, et al. 2023. ‚ÄúBeyond Human Data:
Scaling Self-Training for Problem-Solving with Language Models.‚Äù *arXiv
\[Cs.LG\]*. <http://arxiv.org/abs/2312.06585>.

Snell, Charlie, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. ‚ÄúScaling
LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model
Parameters.‚Äù *arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2408.03314>.

Su, Dijia, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and
Qinqing Zheng. 2024. ‚ÄúDualformer: Controllable Fast and Slow Thinking by
Learning with Randomized Reasoning Traces.‚Äù *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2410.09918>.

Uesato, Jonathan, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel,
Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022.
‚ÄúSolving Math Word Problems with Process- and Outcome-Based Feedback.‚Äù
*arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2211.14275>.

Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024.
‚ÄúMixture-of-Agents Enhances Large Language Model Capabilities.‚Äù *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2406.04692>.

Wang, Peiyi, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli
Chen, Y Wu, and Zhifang Sui. 2023. ‚ÄúMath-Shepherd: Verify and Reinforce
LLMs Step-by-Step Without Human Annotations.‚Äù *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2312.08935>.

Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. ‚ÄúSelf-Consistency
Improves Chain of Thought Reasoning in Language Models.‚Äù *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2203.11171>.

Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. ‚ÄúChain-of-Thought
Prompting Elicits Reasoning in Large Language Models.‚Äù Edited by S
Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, and A Oh. *arXiv
\[Cs.CL\]*, 24824‚Äì37.
<https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf>.

Welleck, Sean, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf,
Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. ‚ÄúFrom
Decoding to Meta-Generation: Inference-Time Algorithms for Large
Language Models.‚Äù *arXiv \[Cs.CL\]*. <http://arxiv.org/abs/2406.16838>.

Wu, Tianhao, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and
Sainbayar Sukhbaatar. 2024. ‚ÄúThinking LLMs: General Instruction
Following with Thought Generation.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2410.10630>.

Wu, Yangzhen, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
2024. ‚ÄúInference Scaling Laws: An Empirical Analysis of Compute-Optimal
Inference for Problem-Solving with Language Models.‚Äù *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2408.00724>.

Xie, Yuxi, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P
Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. ‚ÄúMonte Carlo Tree
Search Boosts Reasoning via Iterative Preference Learning.‚Äù *arXiv
\[Cs.AI\]*. <http://arxiv.org/abs/2405.00451>.

Xie, Yuxi, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian
He, and Qizhe Xie. 2023. ‚ÄúSelf-Evaluation Guided Beam Search for
Reasoning.‚Äù *arXiv \[Cs.CL\]*. <http://arxiv.org/abs/2305.00633>.

Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
Yuan Cao, and Karthik Narasimhan. 2023. ‚ÄúTree of Thoughts: Deliberate
Problem Solving with Large Language Models.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2305.10601>.

Yarowsky, David. 1995. ‚ÄúUnsupervised Word Sense Disambiguation Rivaling
Supervised Methods.‚Äù In *Proceedings of the 33rd Annual Meeting on
Association for Computational Linguistics -*. Morristown, NJ, USA:
Association for Computational Linguistics.
<https://dl.acm.org/doi/10.3115/981658.981684>.

Yoshida, Davis, Kartik Goyal, and Kevin Gimpel. 2024.
‚Äú<span class="nocase">MAP‚Äôs</span> Not Dead yet: Uncovering True
Language Model Modes by Conditioning Away Degeneracy.‚Äù In *Proceedings
of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)*, 16164‚Äì215. Stroudsburg, PA, USA:
Association for Computational Linguistics.
<https://aclanthology.org/2024.acl-long.855.pdf>.

Zelikman, Eric, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber,
and Noah D Goodman. 2024. ‚ÄúQuiet-STaR: Language Models Can Teach
Themselves to Think Before Speaking.‚Äù *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2403.09629>.

Zelikman, Eric, Yuhuai Wu, Jesse Mu, and Noah D Goodman. 2022. ‚ÄúSTaR:
Bootstrapping Reasoning with Reasoning.‚Äù *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2203.14465>.

Zhao, Stephen, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. 2024.
‚ÄúProbabilistic Inference in Language Models via Twisted Sequential Monte
Carlo.‚Äù *arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2404.17546>.

- [üåü Contributors](#-contributors)
- [Star History](#star-history)

---

# Resources

## Workshops and Tutorials

<table>
    <tr>
        <th>Theme</th>
        <th>Source</th>
        <th>Link</th>
        <th>Other</th>
    </tr>
    <tr>
        <th>‚Ä¶‚Ä¶</th>
        <th>‚Ä¶‚Ä¶</th>
        <th>‚Ä¶‚Ä¶</th>
        <th>‚Ä¶‚Ä¶</th>
    </tr>
    <tr>
        <th colspan="1">Descriptions</th>
        <td colspan="3">‚Ä¶‚Ä¶</td>
    </tr>
</table>

# Papers

## STaR (Zelikman et al. 2022) Formulates LLM improvement as retraining on rationales that lead to correct answers. Justified as approximate policy gradient.
STaR (Zelikman et al. 2022) Formulates LLM improvement as retraining on rationales that lead to correct answers. Justified as approximate policy gradient.
1. Data Benchmark
Dataset: The STaR (Self-Taught Reasoner) paper primarily evaluates its model on the CommonsenseQA dataset. CommonsenseQA is a challenging benchmark that tests models on multiple-choice questions requiring commonsense reasoning. Each question has a correct answer that can only be identified by understanding real-world knowledge or relationships.
Performance Comparison: STaR‚Äôs performance is compared against several baselines, including:
A few-shot prompting baseline (+35.9% improvement over zero-shot).
State-of-the-art language models trained with large, annotated datasets for reasoning tasks.
Significance: The paper demonstrates that STaR achieves competitive results on CommonsenseQA, achieving around 72.5% accuracy, which is comparable to models fine-tuned on large-scale annotated datasets. STaR‚Äôs method only uses few-shot examples, making it highly efficient.
2. Algorithm
Self-Taught Reasoning (STaR): The core algorithm is based on a bootstrapping loop that leverages a model‚Äôs existing reasoning ability to iteratively generate rationales (explanations) for each question. Here‚Äôs how it works:

Initial Loop: The model is given a few example rationales as a starting point and is prompted to generate rationales for other questions.
Generation and Fine-Tuning: For each question, the model attempts to generate a rationale and predict the answer. If the generated answer is correct, that rationale is saved; if not, the model is given the correct answer and asked to generate a rationale for it. This process is called rationalization.
Iterative Fine-Tuning: The saved rationales from correct answers are added to the training data. The model is then fine-tuned iteratively on this growing dataset, allowing it to improve over time by learning from its own generated rationales.
Rationale Collection and Repetition: This bootstrapping loop repeats, gradually improving the model‚Äôs reasoning accuracy and enabling it to solve increasingly complex reasoning tasks.
Key Advantages: By generating rationales for both correct and corrected answers, STaR sidesteps the need for large annotated datasets. Instead, it leverages its generated rationales as pseudo-labels to refine its reasoning ability.

3. Input and Output
Input: The input to STaR consists of questions requiring commonsense reasoning, each with multiple-choice answers. Additionally, a small set of few-shot examples with rationale-based explanations is used to prime the model initially.
Output:
Generated Rationale: For each question, the model generates a step-by-step rationale (chain of thought) explaining its reasoning process.
Predicted Answer: Based on the rationale, the model selects the most likely answer from the multiple choices.
Output Format Example:
Question: "What can be used to carry a small dog? (Choices: (a) swimming pool, (b) cake box, (c) backpack, (d) toy)"
Generated Rationale: "A cake box or toy may not be practical for carrying a dog. A backpack can carry a small dog securely and is designed to hold things."
Answer: "(c) backpack"



# üåü Contributors

<a href="">
  <img src="" />
</a>

# Star History

[![Star History Chart]()
